import torch
from torch import nn
from torch.nn import GRU, LSTM, Embedding

from f21nl.interfaces import CellType


class NMTEncoder(nn.Module):
    """A simple sequence encoder using either GRU or LSTM."""

    def __init__(
        self,
        encoder_cell_type: CellType,
        encoder_input_dim: int,
        encoder_output_dim: int,
        source_vocab_size: int,
        source_embedding_dim: int,
        bidirectional: bool = False,
    ) -> None:
        super().__init__()
        self._encoder_input_dim = encoder_input_dim
        self._encoder_output_dim = encoder_output_dim
        self._bidirectional = bidirectional

        self.embeddings = Embedding(source_vocab_size, source_embedding_dim)

        if encoder_cell_type == "gru":
            self._encoder = GRU(
                self._encoder_input_dim,
                self._encoder_output_dim,
                bidirectional=self._bidirectional,
            )
        elif encoder_cell_type == "lstm":
            self._encoder = LSTM(
                self._encoder_input_dim,
                self._encoder_output_dim,
                bidirectional=self._bidirectional,
            )
        else:
            raise ValueError(f"Dialogue encoder of type {encoder_cell_type} not supported yet!")

    def forward(self, encoder_input_ids: torch.Tensor) -> dict[str, torch.Tensor]:
        """Performs the forward pass over the input sequence."""
        encoder_inputs = self.embeddings(encoder_input_ids)

        return {"encoder_outputs": self._encoder(encoder_inputs)[0]}

    def get_final_encoder_states(
        self, encoder_outputs: torch.Tensor, mask: torch.BoolTensor
    ) -> torch.Tensor:
        """Returns the last valid hidden states generated by an RNN-based model.

        Given the output from a `Seq2SeqEncoder`, with shape `(batch_size, sequence_length,
        encoding_dim)`, this method returns the final hidden state for each element of the batch,
        giving a tensor of shape `(batch_size, encoding_dim)`.  This is not as simple as
        `encoder_outputs[:, -1]`, because the sequences could have different lengths.  We use the
        mask (which has shape `(batch_size, sequence_length)`) to find the final state for each batch
        instance.

        Additionally, if `self._bidirectional` is `True`, we will split the final dimension of the
        `encoder_outputs` into two and assume that the first half is for the forward direction of the
        encoder and the second half is for the backward direction.  We will concatenate the last state
        for each encoder dimension, giving `encoder_outputs[:, -1, :encoding_dim/2]` concatenated with
        `encoder_outputs[:, 0, encoding_dim/2:]`.

        Credit: https://github.com/allenai/allennlp/blob/80fb6061e568cb9d6ab5d45b661e86eb61b92c82/allennlp/nn/util.py#L181C1-L211C32
        """
        # These are the indices of the last words in the sequences (i.e. length sans padding - 1).  We
        # are assuming sequences are right padded.
        # Shape: (batch_size,)
        last_word_indices = mask.sum(1) - 1
        batch_size, _, encoder_output_dim = encoder_outputs.size()
        expanded_indices = last_word_indices.view(-1, 1, 1).expand(
            batch_size, 1, encoder_output_dim
        )
        # Shape: (batch_size, 1, encoder_output_dim)
        final_encoder_output = encoder_outputs.gather(1, expanded_indices)
        final_encoder_output = final_encoder_output.squeeze(1)  # (batch_size, encoder_output_dim)
        if self._bidirectional:
            final_forward_output = final_encoder_output[:, : (encoder_output_dim // 2)]
            final_backward_output = encoder_outputs[:, 0, (encoder_output_dim // 2) :]
            final_encoder_output = torch.cat([final_forward_output, final_backward_output], dim=-1)
        return final_encoder_output
